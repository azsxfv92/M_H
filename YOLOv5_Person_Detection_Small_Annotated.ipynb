{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a1",
   "metadata": {},
   "source": [
    "# YOLOv5 Person Detection - COCO Dataset (1/10 ÏÉòÌîå)\n",
    "\n",
    "This notebook trains YOLOv5 person detection model using 1/10 of COCO dataset.\n",
    "\n",
    "**Note**: Uses inference-based evaluation instead of validation loss calculation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2",
   "metadata": {},
   "source": [
    "## 1. Setup and Library Import"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91f2986b",
   "metadata": {},
   "source": [
    "## üì¶ Import Libraries\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "‚úì ÎùºÏù¥Î∏åÎü¨Î¶¨ ÏûÑÌè¨Ìä∏ ÏôÑÎ£å\n"
     ]
    }
   ],
   "source": [
    "import argparse\n",
    "import os\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from tqdm import tqdm\n",
    "import yaml\n",
    "from pathlib import Path\n",
    "\n",
    "from utils.dataloaders import create_dataloader\n",
    "from utils.general import check_dataset\n",
    "from utils.torch_utils import select_device\n",
    "from models.yolo import Model\n",
    "from utils.loss import ComputeLoss\n",
    "\n",
    "print(\"‚úì Libraries imported\")\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8a4400c",
   "metadata": {},
   "source": [
    "## Dataset Preprocessing Function\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter images with targets only\n",
    "def filter_images_with_labels(img_dir, label_dir):\n",
    "    \"\"\"Return image paths with targets only\"\"\"\n",
    "    img_dir = Path(img_dir)\n",
    "    label_dir = Path(label_dir)\n",
    "    \n",
    "    valid_images = []\n",
    "    all_images = sorted(img_dir.glob('*.jpg'))\n",
    "    \n",
    "    print(f\"Filtering images...\")\n",
    "    for img_path in tqdm(all_images, desc='Checking labels'):\n",
    "        # Label file path\n",
    "        label_path = label_dir / f\"{img_path.stem}.txt\"\n",
    "        \n",
    "        # Include if label file exists and not empty\n",
    "        if label_path.exists():\n",
    "            with open(label_path, 'r') as f:\n",
    "                content = f.read().strip()\n",
    "                if content:  # If has content\n",
    "                    valid_images.append(str(img_path))\n",
    "    \n",
    "    return valid_images\n",
    "\n",
    "# Train: Filter images with targets only\n",
    "train_img_dir = '../datasets/coco/images/train2017'\n",
    "train_label_dir = '../datasets/coco/labels/train2017'\n",
    "\n",
    "print(\"\\n[Train] Filtering images with person...\")\n",
    "train_images = filter_images_with_labels(train_img_dir, train_label_dir)\n",
    "total_train_imgs = len(list(Path(train_img_dir).glob('*.jpg')))\n",
    "print(f\"  Total images: {total_train_imgs}\")\n",
    "print(f\"  With person: {len(train_images)}\")\n",
    "print(f\"  Filter ratio: {100 * len(train_images) / total_train_imgs:.1f}%\")\n",
    "\n",
    "# Val: Filter images that contain the target\n",
    "val_img_dir = '../datasets/coco/images/val2017'\n",
    "val_label_dir = '../datasets/coco/labels/val2017'\n",
    "\n",
    "print(\"\\n[Val] Filtering images with person...\")\n",
    "val_images = filter_images_with_labels(val_img_dir, val_label_dir)\n",
    "total_val_imgs = len(list(Path(val_img_dir).glob('*.jpg')))\n",
    "print(f\"  Total images: {total_val_imgs}\")\n",
    "print(f\"  With person: {len(val_images)}\")\n",
    "print(f\"  Filter ratio: {100 * len(val_images) / total_val_imgs:.1f}%\")\n",
    "\n",
    "# Save filtered image list to text file\n",
    "train_list_file = '../datasets/coco/train2017_with_person.txt'\n",
    "val_list_file = '../datasets/coco/val2017_with_person.txt'\n",
    "\n",
    "with open(train_list_file, 'w') as f:\n",
    "    f.write('\\n'.join(train_images))\n",
    "\n",
    "with open(val_list_file, 'w') as f:\n",
    "    f.write('\\n'.join(val_images))\n",
    "\n",
    "print(f\"\\n‚úì Image list saved\")\n",
    "\n",
    "# Train dataloader - use filtered images only\n",
    "train_loader_full, train_dataset = create_dataloader(\n",
    "    path=train_list_file,  # pass as txt file\n",
    "    imgsz=640,\n",
    "    batch_size=args.batch_size,\n",
    "    stride=32,\n",
    "    single_cls=True,\n",
    "    hyp=None,\n",
    "    augment=False,\n",
    "    cache=False,\n",
    "    rect=False,\n",
    "    rank=-1,\n",
    "    workers=args.num_workers,\n",
    "    shuffle=True,\n",
    "    prefix='train: '\n",
    ")\n",
    "\n",
    "# Val dataloader - use filtered images only\n",
    "val_loader_full, val_dataset = create_dataloader(\n",
    "    path=val_list_file,  # pass as txt file\n",
    "    imgsz=640,\n",
    "    batch_size=args.batch_size,\n",
    "    stride=32,\n",
    "    single_cls=True,\n",
    "    hyp=None,\n",
    "    augment=False,\n",
    "    cache=False,\n",
    "    rect=False,\n",
    "    rank=-1,\n",
    "    workers=args.num_workers,\n",
    "    shuffle=False,\n",
    "    prefix='val: '\n",
    ")\n",
    "\n",
    "# 1/10 sampling (from full data)\n",
    "train_sampled_size = len(train_dataset) // 10\n",
    "val_sampled_size = len(val_dataset) // 10\n",
    "\n",
    "train_indices = list(range(0, len(train_dataset), 10))  # 1 out of every 10\n",
    "val_indices = list(range(0, len(val_dataset), 10))\n",
    "\n",
    "from torch.utils.data import Subset\n",
    "train_subset = Subset(train_dataset, train_indices)\n",
    "val_subset = Subset(val_dataset, val_indices)\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_subset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=True,\n",
    "    num_workers=args.num_workers,\n",
    "    collate_fn=train_dataset.collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    val_subset,\n",
    "    batch_size=args.batch_size,\n",
    "    shuffle=False,\n",
    "    num_workers=args.num_workers,\n",
    "    collate_fn=val_dataset.collate_fn,\n",
    "    pin_memory=True\n",
    ")\n",
    "\n",
    "print(f\"\\nDataloader ready\")\n",
    "print(f\"  Train total: {len(train_dataset)} ‚Üí Sampled: {len(train_subset)} (1/10)\")\n",
    "print(f\"  Val total: {len(val_dataset)} ‚Üí Sampled: {len(val_subset)} (1/10)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b5c5ad0d",
   "metadata": {},
   "source": [
    "## Baseline Model Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "f53c45f5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain: \u001b[0mweights=yolov5s.pt, cfg=, data=data/person_final.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=10, batch_size=16, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=4, project=runs/train, name=people_detect_run, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 2 commits. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
      "YOLOv5 üöÄ v7.0-448-gdeec5e45 Python-3.9.23 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 3090 Ti, 24090MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.01, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "Model summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 343/349 items from yolov5s.pt\n",
      "/home/minhyuk/Yolov5_nano/yolov5/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "/home/minhyuk/Yolov5_nano/yolov5/models/common.py:898: FutureWarning: `torch.cuda.amp.autocast(args...)` is deprecated. Please use `torch.amp.autocast('cuda', args...)` instead.\n",
      "  with amp.autocast(autocast):\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.01) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/minhyuk/Yolov5_nano/datasets/coco/person_only_train2017.ca\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/minhyuk/Yolov5_nano/datasets/coco/person_only_val2017.cache.\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.46 anchors/target, 0.997 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
      "Plotting labels to runs/train/people_detect_run2/labels.jpg... \n",
      "/home/minhyuk/Yolov5_nano/yolov5/train.py:357: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  scaler = torch.cuda.amp.GradScaler(enabled=amp)\n",
      "Image sizes 640 train, 640 val\n",
      "Using 4 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/people_detect_run2\u001b[0m\n",
      "Starting training for 10 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        0/9      3.31G    0.05276    0.03722          0         41        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.798      0.668      0.763      0.479\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        1/9      5.54G    0.04293    0.03563          0         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.795      0.673      0.763      0.482\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        2/9      5.54G     0.0428    0.03706          0         40        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.792      0.652      0.746      0.466\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        3/9      5.56G    0.04249    0.03773          0         19        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.796      0.673      0.761      0.483\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        4/9      5.56G    0.04172    0.03732          0         29        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.808      0.669      0.768      0.494\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        5/9      5.56G    0.04082    0.03685          0         16        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.801      0.687      0.777      0.503\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        6/9      5.56G    0.04002    0.03606          0         12        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.817      0.681      0.783      0.511\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        7/9      5.56G    0.03935    0.03542          0         11        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.805      0.691      0.788      0.519\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        8/9      5.56G     0.0386    0.03513          0         31        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.807      0.698      0.791      0.523\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "        9/9      5.56G    0.03782    0.03444          0         18        640: 1\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.818      0.687      0.792      0.526\n",
      "\n",
      "10 epochs completed in 0.753 hours.\n",
      "Optimizer stripped from runs/train/people_detect_run2/weights/last.pt, 14.3MB\n",
      "Optimizer stripped from runs/train/people_detect_run2/weights/best.pt, 14.3MB\n",
      "\n",
      "Validating runs/train/people_detect_run2/weights/best.pt...\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.816      0.689      0.793      0.526\n",
      "Results saved to \u001b[1mruns/train/people_detect_run2\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py --img 640 --batch 16 --epoch 10 --data data/person_final.yaml --weights yolov5s.pt --workers 4 --name people_detect_run "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10",
   "metadata": {},
   "source": [
    "## Apply Pruning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b719a7b-0078-4bb3-afd6-58a4401f3542",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "============================================================\n",
      " PRUNING \n",
      "============================================================\n",
      "\n",
      "Pruning with sensitivity=0.7\n",
      "======================================================================\n",
      "model.0.conv                                  |  70.37% |   2,432/  3,456\n",
      "model.1.conv                                  |  71.10% |  13,106/ 18,432\n",
      "model.2.cv1.conv                              |  73.29% |   1,501/  2,048\n",
      "model.2.cv2.conv                              |  70.26% |   1,439/  2,048\n",
      "model.2.cv3.conv                              |  69.92% |   2,864/  4,096\n",
      "model.2.m.0.cv1.conv                          |  69.43% |     711/  1,024\n",
      "model.2.m.0.cv2.conv                          |  71.27% |   6,568/  9,216\n",
      "model.3.conv                                  |  61.47% |  45,323/ 73,728\n",
      "model.4.cv1.conv                              |  58.42% |   4,786/  8,192\n",
      "model.4.cv2.conv                              |  64.84% |   5,312/  8,192\n",
      "model.4.cv3.conv                              |  59.55% |   9,757/ 16,384\n",
      "model.4.m.0.cv1.conv                          |  52.93% |   2,168/  4,096\n",
      "model.4.m.0.cv2.conv                          |  58.38% |  21,523/ 36,864\n",
      "model.4.m.1.cv1.conv                          |  56.76% |   2,325/  4,096\n",
      "model.4.m.1.cv2.conv                          |  57.40% |  21,160/ 36,864\n",
      "model.5.conv                                  |  57.70% | 170,174/294,912\n",
      "model.6.cv1.conv                              |  55.17% |  18,079/ 32,768\n",
      "model.6.cv2.conv                              |  56.28% |  18,442/ 32,768\n",
      "model.6.cv3.conv                              |  56.36% |  36,939/ 65,536\n",
      "model.6.m.0.cv1.conv                          |  52.86% |   8,660/ 16,384\n",
      "model.6.m.0.cv2.conv                          |  55.89% |  82,412/147,456\n",
      "model.6.m.1.cv1.conv                          |  54.68% |   8,959/ 16,384\n",
      "model.6.m.1.cv2.conv                          |  55.36% |  81,630/147,456\n",
      "model.6.m.2.cv1.conv                          |  53.78% |   8,811/ 16,384\n",
      "model.6.m.2.cv2.conv                          |  55.92% |  82,452/147,456\n",
      "model.7.conv                                  |  54.64% | 644,544/1,179,648\n",
      "model.8.cv1.conv                              |  54.84% |  71,875/131,072\n",
      "model.8.cv2.conv                              |  52.33% |  68,596/131,072\n",
      "model.8.cv3.conv                              |  55.13% | 144,509/262,144\n",
      "model.8.m.0.cv1.conv                          |  54.55% |  35,748/ 65,536\n",
      "model.8.m.0.cv2.conv                          |  55.54% | 327,581/589,824\n",
      "model.9.cv1.conv                              |  53.45% |  70,054/131,072\n",
      "model.9.cv2.conv                              |  59.05% | 309,601/524,288\n",
      "model.10.conv                                 |  56.67% |  74,272/131,072\n",
      "model.13.cv1.conv                             |  60.30% |  39,518/ 65,536\n",
      "model.13.cv2.conv                             |  56.22% |  36,844/ 65,536\n",
      "model.13.cv3.conv                             |  62.00% |  40,631/ 65,536\n",
      "model.13.m.0.cv1.conv                         |  58.60% |   9,601/ 16,384\n",
      "model.13.m.0.cv2.conv                         |  62.66% |  92,392/147,456\n",
      "model.14.conv                                 |  63.71% |  20,876/ 32,768\n",
      "model.17.cv1.conv                             |  66.70% |  10,928/ 16,384\n",
      "model.17.cv2.conv                             |  67.19% |  11,008/ 16,384\n",
      "model.17.cv3.conv                             |  71.11% |  11,650/ 16,384\n",
      "model.17.m.0.cv1.conv                         |  63.92% |   2,618/  4,096\n",
      "model.17.m.0.cv2.conv                         |  64.56% |  23,799/ 36,864\n",
      "model.18.conv                                 |  73.79% | 108,804/147,456\n",
      "model.20.cv1.conv                             |  69.73% |  22,849/ 32,768\n",
      "model.20.cv2.conv                             |  66.64% |  21,835/ 32,768\n",
      "model.20.cv3.conv                             |  73.55% |  48,202/ 65,536\n",
      "model.20.m.0.cv1.conv                         |  64.93% |  10,638/ 16,384\n",
      "model.20.m.0.cv2.conv                         |  68.48% | 100,979/147,456\n",
      "model.21.conv                                 |  76.00% | 448,295/589,824\n",
      "model.23.cv1.conv                             |  68.81% |  90,190/131,072\n",
      "model.23.cv2.conv                             |  69.68% |  91,337/131,072\n",
      "model.23.cv3.conv                             |  72.85% | 190,981/262,144\n",
      "model.23.m.0.cv1.conv                         |  67.30% |  44,105/ 65,536\n",
      "model.23.m.0.cv2.conv                         |  70.29% | 414,566/589,824\n",
      "model.24.m.0                                  |  79.17% |   1,824/  2,304\n",
      "model.24.m.1                                  |  82.64% |   3,808/  4,608\n",
      "model.24.m.2                                  |  78.84% |   7,266/  9,216\n",
      "======================================================================\n",
      "TOTAL                                         |  61.54% | 4,309,857/7,003,264\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch.nn as nn\n",
    "SENSITIVITY = 0.7\n",
    " \n",
    "def prune_by_std(model, s):\n",
    "\n",
    "    pruned_count = 0\n",
    "    total_count = 0\n",
    "    \n",
    "    print(f\"\\nPruning with sensitivity={s}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, nn.Conv2d):\n",
    "            weight = module.weight.data\n",
    "            std = weight.std().item()\n",
    "            threshold = std * s\n",
    "            mask = weight.abs() < threshold\n",
    "            weight[mask] = 0.0\n",
    "            \n",
    "            layer_pruned = mask.sum().item()\n",
    "            layer_total = weight.numel()\n",
    "            pruned_count += layer_pruned\n",
    "            total_count += layer_total\n",
    "            \n",
    "            ratio = 100.0 * layer_pruned / layer_total\n",
    "            print(f\"{name:<45} | {ratio:>6.2f}% | {layer_pruned:>7,}/{layer_total:>7,}\")\n",
    "    \n",
    "    overall = 100.0 * pruned_count / total_count\n",
    "    print(\"=\"*70)\n",
    "    print(f\"{'TOTAL':<45} | {overall:>6.2f}% | {pruned_count:>7,}/{total_count:>7,}\")\n",
    "    print(\"=\"*70)\n",
    "    \n",
    "    return pruned_count, total_count\n",
    "\n",
    "print(\"\\n\" + \"=\"*60)\n",
    "print(\" PRUNING \")\n",
    "print(\"=\"*60)\n",
    "\n",
    "ckpt = torch.load('runs/train/exp2/weights/best.pt', map_location = device, weights_only=False)\n",
    "best_model = ckpt['model'].float().to(device)\n",
    "\n",
    "# perform Pruning \n",
    "pruned, total = prune_by_std(best_model, SENSITIVITY)\n",
    "\n",
    "save_ckpt = {\n",
    "    'model': best_model,  \n",
    "    'epoch': ckpt.get('epoch', -1),\n",
    "    'best_fitness': ckpt.get('best_fitness', None),\n",
    "    'optimizer': None,  \n",
    "    'date': 'Pruned Model Checkpoint',\n",
    "    'ema': None,  \n",
    "}\n",
    "\n",
    "# torch.save(save_ckpt, 'runs/pruned/best_pruned.pt')\n",
    "torch.save(save_ckpt, 'runs/pruned/best_pruned_70.pt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13063128",
   "metadata": {},
   "source": [
    "## Validate Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "e387bbef",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=data/person_final.yaml, weights=['runs/pruned/best_pruned_70.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 üöÄ v7.0-448-gdeec5e45 Python-3.9.23 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 3090 Ti, 24090MiB)\n",
      "\n",
      "Fusing layers... \n",
      "Model summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/minhyuk/Yolov5_nano/datasets/coco/person_only_val2017.cache.\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.109      0.322      0.141     0.0625\n",
      "Speed: 0.1ms pre-process, 1.1ms inference, 0.3ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp6\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights runs/pruned/best_pruned_70.pt --data data/person_final.yaml --img 640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "12ef45c4",
   "metadata": {},
   "source": [
    "## Fine-tune Pruned Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "dffd0936",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain_fine_tuning: \u001b[0mweights=runs/pruned/best_pruned_70.pt, cfg=models/yolov5s.yaml, data=data/person_final.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=5, batch_size=64, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=pruned_finetuned, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 2 commits. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
      "YOLOv5 üöÄ v7.0-448-gdeec5e45 Python-3.9.23 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 3090 Ti, 24090MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/pruned/best_pruned_70.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0001) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/minhyuk/Yolov5_nano/datasets/coco/person_only_train2017.ca\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/minhyuk/Yolov5_nano/datasets/coco/person_only_val2017.cache.\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.46 anchors/target, 0.997 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
      "Plotting labels to runs/train/pruned_finetuned13/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/pruned_finetuned13\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "Epoch [1/5] 100.0% (51102/51102 imgs) | Loss: 0.0786 | Mem: 1.83G: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.791      0.661      0.752      0.474\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "Epoch [2/5] 100.0% (51102/51102 imgs) | Loss: 0.0767 | Mem: 12.9G: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.797      0.667      0.761      0.483\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "Epoch [3/5] 100.0% (51102/51102 imgs) | Loss: 0.0748 | Mem: 18.1G: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.809      0.666      0.766       0.49\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "Epoch [4/5] 100.0% (51102/51102 imgs) | Loss: 0.0740 | Mem: 18.1G: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777       0.81       0.67      0.769      0.494\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "Epoch [5/5] 100.0% (51102/51102 imgs) | Loss: 0.0734 | Mem: 18.1G: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.808      0.675      0.771      0.497\n",
      "\n",
      "5 epochs completed in 0.307 hours.\n",
      "Optimizer stripped from runs/train/pruned_finetuned13/weights/last.pt, 28.3MB\n",
      "Optimizer stripped from runs/train/pruned_finetuned13/weights/best.pt, 28.3MB\n",
      "\n",
      "Validating runs/train/pruned_finetuned13/weights/best.pt...\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.809      0.674      0.771      0.497\n",
      "Results saved to \u001b[1mruns/train/pruned_finetuned13\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train_fine_tuning.py --weights runs/pruned/best_pruned_70.pt --cfg models/yolov5s.yaml --data data/person_final.yaml --epochs 5 --batch-size 64 --hyp data/hyps/hyp.scratch-low.yaml --workers 8 --name pruned_finetuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07e2ef67",
   "metadata": {},
   "source": [
    "# Apply Quantization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "4a20cec9-d613-47a1-ab83-4c35a6f61f1c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[‚úì] model.0.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.2.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.2.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.2.cv3.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.2.m.0.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.2.m.0.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.3.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.4.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.4.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.4.cv3.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.4.m.0.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.4.m.0.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.4.m.1.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.4.m.1.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.5.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.6.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.6.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.6.cv3.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.6.m.0.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.6.m.0.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.6.m.1.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.6.m.1.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.6.m.2.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.6.m.2.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.7.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.8.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.8.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.8.cv3.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.8.m.0.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.8.m.0.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.9.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.9.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.10.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.13.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.13.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.13.cv3.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.13.m.0.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.13.m.0.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.14.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.17.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.17.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.17.cv3.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.17.m.0.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.17.m.0.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.18.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.20.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.20.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.20.cv3.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.20.m.0.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.20.m.0.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.21.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.23.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.23.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.23.cv3.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.23.m.0.cv1.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.23.m.0.cv2.conv quantized to 32 levels (5-bit)\n",
      "[‚úì] model.24.m.0 quantized to 32 levels (5-bit)\n",
      "[‚úì] model.24.m.1 quantized to 32 levels (5-bit)\n",
      "[‚úì] model.24.m.2 quantized to 32 levels (5-bit)\n",
      "‚úÖ Weight sharing applied to YOLOv5 successfully!\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "from scipy.sparse import csr_matrix, csc_matrix\n",
    "import copy\n",
    "\n",
    "def apply_weight_sharing(model, bits):\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n",
    "            \n",
    "            dev = module.weight.device\n",
    "            weight = module.weight.data.cpu().numpy()\n",
    "\n",
    "            original_shape = weight.shape\n",
    "            flat_weight = weight.reshape(-1,1)\n",
    "\n",
    "            min_ = min(flat_weight)\n",
    "            max_ = max(flat_weight)\n",
    "            n_clusters = 2**bits\n",
    "            init_points = np.linspace(min_, max_, num=n_clusters).reshape(-1,1)\n",
    "            kmeans = KMeans(\n",
    "                n_clusters=n_clusters,\n",
    "                init=init_points,\n",
    "                n_init=1,\n",
    "                algorithm=\"lloyd\",\n",
    "                max_iter=10,\n",
    "                random_state=42\n",
    "            ).fit(flat_weight)\n",
    "\n",
    "            new_weight = kmeans.cluster_centers_[kmeans.labels_].reshape(original_shape)\n",
    "\n",
    "            module.weight.data = torch.from_numpy(new_weight).to(dev)\n",
    "            print(f\"[‚úì] {name} quantized to {n_clusters} levels ({bits}-bit)\")\n",
    "\n",
    "    print(\"‚úÖ Weight sharing applied to YOLOv5 successfully!\")\n",
    "    return model\n",
    "            \n",
    "ckpt = torch.load('runs/train/pruned_finetuned13/weights/best.pt', map_location=device, weights_only=False)\n",
    "pruned_model = ckpt['model'].float().to(device)\n",
    "quantized_model = copy.deepcopy(pruned_model)\n",
    "quantized_model = apply_weight_sharing(quantized_model, 5)\n",
    "torch.save({'model':quantized_model}, 'runs/pruned/quantized_best_70.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e60e64f2",
   "metadata": {},
   "source": [
    "## Weight Sharing \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf68f437-893e-42b1-8f5a-5e6c84c39b79",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "======================================================================\n",
      "ÌîÑÎ£®Îãù Î≥¥Ï°¥ ÏñëÏûêÌôî (Pruning-aware Quantization)\n",
      "======================================================================\n",
      "\n",
      "ÏñëÏûêÌôî Ï†Ñ Sparsity: 61.46%\n",
      "\n",
      "[‚úì] model.0.conv quantized to 32 levels (5-bit) | Sparsity: 70.37%\n",
      "[‚úì] model.1.conv quantized to 32 levels (5-bit) | Sparsity: 71.10%\n",
      "[‚úì] model.2.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 73.29%\n",
      "[‚úì] model.2.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 70.26%\n",
      "[‚úì] model.2.cv3.conv quantized to 32 levels (5-bit) | Sparsity: 69.92%\n",
      "[‚úì] model.2.m.0.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 69.43%\n",
      "[‚úì] model.2.m.0.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 71.27%\n",
      "[‚úì] model.3.conv quantized to 32 levels (5-bit) | Sparsity: 61.47%\n",
      "[‚úì] model.4.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 58.42%\n",
      "[‚úì] model.4.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 64.84%\n",
      "[‚úì] model.4.cv3.conv quantized to 32 levels (5-bit) | Sparsity: 59.55%\n",
      "[‚úì] model.4.m.0.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 52.93%\n",
      "[‚úì] model.4.m.0.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 58.38%\n",
      "[‚úì] model.4.m.1.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 56.76%\n",
      "[‚úì] model.4.m.1.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 57.40%\n",
      "[‚úì] model.5.conv quantized to 32 levels (5-bit) | Sparsity: 57.70%\n",
      "[‚úì] model.6.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 55.17%\n",
      "[‚úì] model.6.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 56.28%\n",
      "[‚úì] model.6.cv3.conv quantized to 32 levels (5-bit) | Sparsity: 56.36%\n",
      "[‚úì] model.6.m.0.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 52.86%\n",
      "[‚úì] model.6.m.0.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 55.89%\n",
      "[‚úì] model.6.m.1.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 54.68%\n",
      "[‚úì] model.6.m.1.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 55.36%\n",
      "[‚úì] model.6.m.2.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 53.78%\n",
      "[‚úì] model.6.m.2.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 55.92%\n",
      "[‚úì] model.7.conv quantized to 32 levels (5-bit) | Sparsity: 54.64%\n",
      "[‚úì] model.8.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 54.84%\n",
      "[‚úì] model.8.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 52.33%\n",
      "[‚úì] model.8.cv3.conv quantized to 32 levels (5-bit) | Sparsity: 55.13%\n",
      "[‚úì] model.8.m.0.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 54.55%\n",
      "[‚úì] model.8.m.0.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 55.54%\n",
      "[‚úì] model.9.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 53.45%\n",
      "[‚úì] model.9.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 59.05%\n",
      "[‚úì] model.10.conv quantized to 32 levels (5-bit) | Sparsity: 56.67%\n",
      "[‚úì] model.13.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 60.30%\n",
      "[‚úì] model.13.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 56.22%\n",
      "[‚úì] model.13.cv3.conv quantized to 32 levels (5-bit) | Sparsity: 62.00%\n",
      "[‚úì] model.13.m.0.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 58.60%\n",
      "[‚úì] model.13.m.0.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 62.66%\n",
      "[‚úì] model.14.conv quantized to 32 levels (5-bit) | Sparsity: 63.71%\n",
      "[‚úì] model.17.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 66.70%\n",
      "[‚úì] model.17.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 67.19%\n",
      "[‚úì] model.17.cv3.conv quantized to 32 levels (5-bit) | Sparsity: 71.11%\n",
      "[‚úì] model.17.m.0.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 63.92%\n",
      "[‚úì] model.17.m.0.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 64.56%\n",
      "[‚úì] model.18.conv quantized to 32 levels (5-bit) | Sparsity: 73.79%\n",
      "[‚úì] model.20.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 69.73%\n",
      "[‚úì] model.20.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 66.64%\n",
      "[‚úì] model.20.cv3.conv quantized to 32 levels (5-bit) | Sparsity: 73.55%\n",
      "[‚úì] model.20.m.0.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 64.93%\n",
      "[‚úì] model.20.m.0.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 68.48%\n",
      "[‚úì] model.21.conv quantized to 32 levels (5-bit) | Sparsity: 76.00%\n",
      "[‚úì] model.23.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 68.81%\n",
      "[‚úì] model.23.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 69.68%\n",
      "[‚úì] model.23.cv3.conv quantized to 32 levels (5-bit) | Sparsity: 72.85%\n",
      "[‚úì] model.23.m.0.cv1.conv quantized to 32 levels (5-bit) | Sparsity: 67.30%\n",
      "[‚úì] model.23.m.0.cv2.conv quantized to 32 levels (5-bit) | Sparsity: 70.29%\n",
      "[‚úì] model.24.m.0 quantized to 32 levels (5-bit) | Sparsity: 79.17%\n",
      "[‚úì] model.24.m.1 quantized to 32 levels (5-bit) | Sparsity: 82.64%\n",
      "[‚úì] model.24.m.2 quantized to 32 levels (5-bit) | Sparsity: 78.84%\n",
      "‚úÖ Weight sharing applied (pruning preserved)!\n",
      "\n",
      "ÏñëÏûêÌôî ÌõÑ Sparsity: 61.46%\n",
      "Sparsity Î≥ÄÌôî: 61.46% ‚Üí 61.46% (Ï∞®Ïù¥: +0.00%)\n",
      "\n",
      "‚úÖ Ï†ÄÏû• ÏôÑÎ£å: runs/pruned/quantized_best_70_preserve_pruning.pt\n",
      "======================================================================\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "from sklearn.cluster import KMeans\n",
    "import copy\n",
    "\n",
    "def apply_weight_sharing_preserve_zero(model, bits):\n",
    "    \"\"\"\n",
    "    Weight quantization preserving pruning\n",
    "    - Keep zero weights as zero\n",
    "    - Cluster only non-zero weights\n",
    "    \"\"\"\n",
    "    for name, module in model.named_modules():\n",
    "        if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n",
    "            \n",
    "            dev = module.weight.device\n",
    "            weight = module.weight.data.cpu().numpy()\n",
    "            original_shape = weight.shape\n",
    "            \n",
    "            # Select only non-zero weights\n",
    "            non_zero_mask = (weight != 0)\n",
    "            non_zero_weights = weight[non_zero_mask]\n",
    "            \n",
    "            if len(non_zero_weights) == 0:\n",
    "                print(f\"[SKIP] {name} - All weights are zero\")\n",
    "                continue\n",
    "            \n",
    "            # Cluster only non-zero weights\n",
    "            flat_weight = non_zero_weights.reshape(-1, 1)\n",
    "            \n",
    "            min_ = np.min(flat_weight)\n",
    "            max_ = np.max(flat_weight)\n",
    "            n_clusters = 2**bits\n",
    "            init_points = np.linspace(min_, max_, num=n_clusters).reshape(-1, 1)\n",
    "            \n",
    "            kmeans = KMeans(\n",
    "                n_clusters=n_clusters,\n",
    "                init=init_points,\n",
    "                n_init=1,\n",
    "                algorithm=\"lloyd\",\n",
    "                max_iter=10,\n",
    "                random_state=42\n",
    "            ).fit(flat_weight)\n",
    "            \n",
    "            # Replace with quantized values\n",
    "            quantized_non_zero = kmeans.cluster_centers_[kmeans.labels_].flatten()\n",
    "            \n",
    "            # Create new weight matrix (keep zeros)\n",
    "            new_weight = weight.copy()\n",
    "            new_weight[non_zero_mask] = quantized_non_zero\n",
    "            \n",
    "            # Apply to model\n",
    "            module.weight.data = torch.from_numpy(new_weight).to(dev)\n",
    "            \n",
    "            # Check sparsity\n",
    "            sparsity = (weight.size - len(non_zero_weights)) / weight.size * 100\n",
    "            print(f\"[‚úì] {name} quantized to {n_clusters} levels ({bits}-bit) | Sparsity: {sparsity:.2f}%\")\n",
    "    \n",
    "    return model\n",
    "\n",
    "print(\"=\" * 70)\n",
    "print(\"Pruning-aware Quantization\")\n",
    "print(\"=\" * 70)\n",
    "\n",
    "ckpt = torch.load('runs/train/pruned_finetuned13/weights/best.pt', map_location=device, weights_only=False)\n",
    "pruned_model = ckpt['model'].float().to(device)\n",
    "\n",
    "# Check sparsity (ÏñëÏûêÌôî Ï†Ñ)\n",
    "total_params = 0\n",
    "zero_params = 0\n",
    "for module in pruned_model.modules():\n",
    "    if hasattr(module, 'weight') and module.weight is not None:\n",
    "        weight = module.weight.data.cpu().numpy()\n",
    "        total_params += weight.size\n",
    "        zero_params += np.sum(weight == 0)\n",
    "        \n",
    "before_sparsity = zero_params / total_params * 100 if total_params > 0 else 0\n",
    "print(f\"\\nSparsity before quantization: {before_sparsity:.2f}%\\n\")\n",
    "\n",
    "# apply quantization\n",
    "quantized_model = copy.deepcopy(pruned_model)\n",
    "quantized_model = apply_weight_sharing_preserve_zero(quantized_model, 5)\n",
    "\n",
    "# Check sparsity (after quantization)\n",
    "total_params = 0\n",
    "zero_params = 0\n",
    "for module in quantized_model.modules():\n",
    "    if hasattr(module, 'weight') and module.weight is not None:\n",
    "        weight = module.weight.data.cpu().numpy()\n",
    "        total_params += weight.size\n",
    "        zero_params += np.sum(weight == 0)\n",
    "        \n",
    "after_sparsity = zero_params / total_params * 100 if total_params > 0 else 0\n",
    "print(f\"\\nSparsity after quantization: {after_sparsity:.2f}%\")\n",
    "print(f\"Sparsity change: {before_sparsity:.2f}% ‚Üí {after_sparsity:.2f}% (diff: {after_sparsity - before_sparsity:+.2f}%)\")\n",
    "\n",
    "# Save\n",
    "torch.save({'model': quantized_model}, 'runs/pruned/quantized_best_70_preserve_pruning.pt')\n",
    "print(f\"\\n‚úÖ Saved: runs/pruned/quantized_best_70_preserve_pruning.pt\")\n",
    "print(\"=\" * 70)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a64b698",
   "metadata": {},
   "source": [
    "## Validate Quantized Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "afd584ec",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mval: \u001b[0mdata=data/person_final.yaml, weights=['runs/pruned/quantized_best_70_preserve_pruning.pt'], batch_size=32, imgsz=640, conf_thres=0.001, iou_thres=0.6, max_det=300, task=val, device=, workers=8, single_cls=False, augment=False, verbose=False, save_txt=False, save_hybrid=False, save_conf=False, save_json=False, project=runs/val, name=exp, exist_ok=False, half=False, dnn=False\n",
      "YOLOv5 üöÄ v7.0-448-gdeec5e45 Python-3.9.23 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 3090 Ti, 24090MiB)\n",
      "\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/minhyuk/Yolov5_nano/datasets/coco/person_only_val2017.cache.\u001b[0m\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.805      0.652      0.755      0.481\n",
      "Speed: 0.1ms pre-process, 1.1ms inference, 0.4ms NMS per image at shape (32, 3, 640, 640)\n",
      "Results saved to \u001b[1mruns/val/exp10\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python val.py --weights runs/pruned/quantized_best_70_preserve_pruning.pt --data data/person_final.yaml --img 640"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "573877b5",
   "metadata": {},
   "source": [
    "## Fine-tune Quantized Model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "132b9fc3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mtrain_fine_tuning: \u001b[0mweights=runs/pruned/quantized_best_70_preserve_pruning.pt, cfg=models/yolov5s.yaml, data=data/person_final.yaml, hyp=data/hyps/hyp.scratch-low.yaml, epochs=5, batch_size=64, imgsz=640, rect=False, resume=False, nosave=False, noval=False, noautoanchor=False, noplots=False, evolve=None, evolve_population=data/hyps, resume_evolve=None, bucket=, cache=None, image_weights=False, device=, multi_scale=False, single_cls=False, optimizer=SGD, sync_bn=False, workers=8, project=runs/train, name=pruned_finetuned, exist_ok=False, quad=False, cos_lr=False, label_smoothing=0.0, patience=100, freeze=[0], save_period=-1, seed=0, local_rank=-1, entity=None, upload_dataset=False, bbox_interval=-1, artifact_alias=latest, ndjson_console=False, ndjson_file=False\n",
      "\u001b[34m\u001b[1mgithub: \u001b[0m‚ö†Ô∏è YOLOv5 is out of date by 2 commits. Use 'git pull' or 'git clone https://github.com/ultralytics/yolov5' to update.\n",
      "YOLOv5 üöÄ v7.0-448-gdeec5e45 Python-3.9.23 torch-2.8.0+cu128 CUDA:0 (NVIDIA GeForce RTX 3090 Ti, 24090MiB)\n",
      "\n",
      "\u001b[34m\u001b[1mhyperparameters: \u001b[0mlr0=0.0001, lrf=0.01, momentum=0.937, weight_decay=0.0005, warmup_epochs=3.0, warmup_momentum=0.8, warmup_bias_lr=0.1, box=0.05, cls=0.5, cls_pw=1.0, obj=1.0, obj_pw=1.0, iou_t=0.2, anchor_t=4.0, fl_gamma=0.0, hsv_h=0.015, hsv_s=0.7, hsv_v=0.4, degrees=0.0, translate=0.1, scale=0.5, shear=0.0, perspective=0.0, flipud=0.0, fliplr=0.5, mosaic=1.0, mixup=0.0, copy_paste=0.0\n",
      "\u001b[34m\u001b[1mComet: \u001b[0mrun 'pip install comet_ml' to automatically track and visualize YOLOv5 üöÄ runs in Comet\n",
      "\u001b[34m\u001b[1mTensorBoard: \u001b[0mStart with 'tensorboard --logdir runs/train', view at http://localhost:6006/\n",
      "Overriding model.yaml nc=80 with nc=1\n",
      "\n",
      "                 from  n    params  module                                  arguments                     \n",
      "  0                -1  1      3520  models.common.Conv                      [3, 32, 6, 2, 2]              \n",
      "  1                -1  1     18560  models.common.Conv                      [32, 64, 3, 2]                \n",
      "  2                -1  1     18816  models.common.C3                        [64, 64, 1]                   \n",
      "  3                -1  1     73984  models.common.Conv                      [64, 128, 3, 2]               \n",
      "  4                -1  2    115712  models.common.C3                        [128, 128, 2]                 \n",
      "  5                -1  1    295424  models.common.Conv                      [128, 256, 3, 2]              \n",
      "  6                -1  3    625152  models.common.C3                        [256, 256, 3]                 \n",
      "  7                -1  1   1180672  models.common.Conv                      [256, 512, 3, 2]              \n",
      "  8                -1  1   1182720  models.common.C3                        [512, 512, 1]                 \n",
      "  9                -1  1    656896  models.common.SPPF                      [512, 512, 5]                 \n",
      " 10                -1  1    131584  models.common.Conv                      [512, 256, 1, 1]              \n",
      " 11                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 12           [-1, 6]  1         0  models.common.Concat                    [1]                           \n",
      " 13                -1  1    361984  models.common.C3                        [512, 256, 1, False]          \n",
      " 14                -1  1     33024  models.common.Conv                      [256, 128, 1, 1]              \n",
      " 15                -1  1         0  torch.nn.modules.upsampling.Upsample    [None, 2, 'nearest']          \n",
      " 16           [-1, 4]  1         0  models.common.Concat                    [1]                           \n",
      " 17                -1  1     90880  models.common.C3                        [256, 128, 1, False]          \n",
      " 18                -1  1    147712  models.common.Conv                      [128, 128, 3, 2]              \n",
      " 19          [-1, 14]  1         0  models.common.Concat                    [1]                           \n",
      " 20                -1  1    296448  models.common.C3                        [256, 256, 1, False]          \n",
      " 21                -1  1    590336  models.common.Conv                      [256, 256, 3, 2]              \n",
      " 22          [-1, 10]  1         0  models.common.Concat                    [1]                           \n",
      " 23                -1  1   1182720  models.common.C3                        [512, 512, 1, False]          \n",
      " 24      [17, 20, 23]  1     16182  models.yolo.Detect                      [1, [[10, 13, 16, 30, 33, 23], [30, 61, 62, 45, 59, 119], [116, 90, 156, 198, 373, 326]], [128, 256, 512]]\n",
      "YOLOv5s summary: 214 layers, 7022326 parameters, 7022326 gradients, 15.9 GFLOPs\n",
      "\n",
      "Transferred 348/349 items from runs/pruned/quantized_best_70_preserve_pruning.pt\n",
      "\u001b[34m\u001b[1mAMP: \u001b[0mchecks passed ‚úÖ\n",
      "\u001b[34m\u001b[1moptimizer:\u001b[0m SGD(lr=0.0001) with parameter groups 57 weight(decay=0.0), 60 weight(decay=0.0005), 60 bias\n",
      "\u001b[34m\u001b[1mtrain: \u001b[0mScanning /home/minhyuk/Yolov5_nano/datasets/coco/person_only_train2017.ca\u001b[0m\n",
      "\u001b[34m\u001b[1mval: \u001b[0mScanning /home/minhyuk/Yolov5_nano/datasets/coco/person_only_val2017.cache.\u001b[0m\n",
      "\n",
      "\u001b[34m\u001b[1mAutoAnchor: \u001b[0m4.46 anchors/target, 0.997 Best Possible Recall (BPR). Current anchors are a good fit to dataset ‚úÖ\n",
      "Plotting labels to runs/train/pruned_finetuned17/labels.jpg... \n",
      "Image sizes 640 train, 640 val\n",
      "Using 8 dataloader workers\n",
      "Logging results to \u001b[1mruns/train/pruned_finetuned17\u001b[0m\n",
      "Starting training for 5 epochs...\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "Epoch [1/5] 100.0% (51102/51102 imgs) | Loss: 0.0749 | Mem: 1.83G: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.801      0.675      0.768      0.493\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "Epoch [2/5] 100.0% (51102/51102 imgs) | Loss: 0.0745 | Mem: 12.9G: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.814      0.671      0.772      0.496\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "Epoch [3/5] 100.0% (51102/51102 imgs) | Loss: 0.0733 | Mem: 18.1G: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.809      0.677      0.772      0.498\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "Epoch [4/5] 100.0% (51102/51102 imgs) | Loss: 0.0730 | Mem: 18.1G: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.813      0.676      0.774        0.5\n",
      "\n",
      "      Epoch    GPU_mem   box_loss   obj_loss   cls_loss  Instances       Size\n",
      "Epoch [5/5] 100.0% (51102/51102 imgs) | Loss: 0.0726 | Mem: 18.1G: 100%|‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà‚ñà\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.811       0.68      0.775      0.501\n",
      "\n",
      "5 epochs completed in 0.307 hours.\n",
      "Optimizer stripped from runs/train/pruned_finetuned17/weights/last.pt, 28.3MB\n",
      "Optimizer stripped from runs/train/pruned_finetuned17/weights/best.pt, 28.3MB\n",
      "\n",
      "Validating runs/train/pruned_finetuned17/weights/best.pt...\n",
      "Fusing layers... \n",
      "YOLOv5s summary: 157 layers, 7012822 parameters, 0 gradients, 15.8 GFLOPs\n",
      "                 Class     Images  Instances          P          R      mAP50   \n",
      "                   all       2693      10777      0.811       0.68      0.775      0.502\n",
      "Results saved to \u001b[1mruns/train/pruned_finetuned17\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train_fine_tuning.py --weights runs/pruned/quantized_best_70_preserve_pruning.pt --cfg models/yolov5s.yaml --data data/person_final.yaml --epochs 5 --batch-size 64 --hyp data/hyps/hyp.scratch-low.yaml --workers 8 --name pruned_finetuned"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c77fd471",
   "metadata": {},
   "source": [
    "# Save as CSR Format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "zysxc8ozkz",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Size of Original model :   26.99 MB\n",
      "Size of Compressed model :   10.94 MB\n",
      "Compression ratio :      59.5%\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pickle\n",
    "import os\n",
    "\n",
    "# 1. Load quantized model\n",
    "ckpt = torch.load('runs/train/pruned_finetuned17/weights/best.pt', map_location=device, weights_only=False)\n",
    "# ckpt = torch.load('runs/train/exp2/weights/best.pt', map_location=device, weights_only=False)\n",
    "\n",
    "model = ckpt['model'].float()\n",
    "\n",
    "# 2. Convert to CSR format\n",
    "csr_model = {}\n",
    "\n",
    "for name, module in model.named_modules():\n",
    "    if isinstance(module, (torch.nn.Conv2d, torch.nn.Linear)):\n",
    "        weight = module.weight.data.cpu().numpy()\n",
    "        \n",
    "        # Save each row (output channel) as CSR\n",
    "        csr_rows = []\n",
    "        for row in weight.reshape(weight.shape[0], -1):\n",
    "            # Index and value of non-zero elements\n",
    "            nz_idx = np.where(row != 0)[0]\n",
    "            nz_val = row[nz_idx]\n",
    "            \n",
    "            # Relative indexing (Delta Encoding)\n",
    "            if len(nz_idx) > 0:\n",
    "                rel_idx = np.diff(nz_idx, prepend=0).astype(np.uint16)\n",
    "            else:\n",
    "                rel_idx = np.array([], dtype=np.uint16)\n",
    "            \n",
    "            csr_rows.append({'idx': rel_idx, 'val': nz_val.astype(np.float16)})\n",
    "        \n",
    "        csr_model[name] = {\n",
    "            'shape': weight.shape,\n",
    "            'rows': csr_rows,\n",
    "            'bias': module.bias.data.cpu().numpy() if module.bias is not None else None\n",
    "        }\n",
    "\n",
    "# 3. Save\n",
    "os.makedirs('runs/compressed', exist_ok=True)\n",
    "with open('runs/compressed/csr_model.pkl', 'wb') as f:\n",
    "    pickle.dump(csr_model, f)\n",
    "\n",
    "# 4. calculate size and compare\n",
    "original_size = os.path.getsize('runs/train/pruned_finetuned17/weights/best.pt')\n",
    "compressed_size = os.path.getsize('runs/compressed/csr_model.pkl')\n",
    "\n",
    "print(f\"Size of Original model :   {original_size / 1024**2:.2f} MB\")\n",
    "print(f\"Size of Compressed model :   {compressed_size / 1024**2:.2f} MB\")\n",
    "print(f\"Compression ratio :      {(1 - compressed_size/original_size)*100:.1f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "yolov5",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
